{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RMTPP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMTPP(nn.Module):\n",
    "    def __init__(self, marker_type='real', marker_dim=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.marker_dim = marker_dim\n",
    "        \n",
    "        # Dimensions for embedding inputs\n",
    "        self.marker_embed_dim = 64\n",
    "        self.time_embed_dim = 64\n",
    "        # Networks for embedding inputs\n",
    "        self.create_embedding_nets()\n",
    "        \n",
    "        # This is the layer that encodes the history\n",
    "        self.hidden_layer_dim = 128\n",
    "        # Create RNN layer Network\n",
    "        self.create_rnn()\n",
    "        \n",
    "        # Hidden shared layer size (bw mu and logvar)\n",
    "        # while generating marker from hidden state\n",
    "        self.marker_shared_dim = 64\n",
    "        # Create Network for Marker generation from hidden_seq\n",
    "        self.create_marker_generation_net()\n",
    "        self.create_time_likelihood_net()\n",
    "\n",
    "    ############ UTILITY METHODS #############\n",
    "        \n",
    "    def _one_hot_marker(self, marker_seq):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            marker_seq: Tensor of shape TxBSx1\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        return mu + epsilon*logvar.exp().sqrt()\n",
    "\n",
    "    \n",
    "    ############ NETWORKS #############    \n",
    "    \n",
    "    def create_rnn(self):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            marker_embed_dim: dimension of embedded markers\n",
    "            time_embed_dim: dimension of embedded times,intervals\n",
    "            hidden_layer_dim: dimension of hidden state of recurrent layer\n",
    "        \"\"\"\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.marker_embed_dim+self.time_embed_dim,\n",
    "            hidden_size=self.hidden_layer_dim,\n",
    "            nonlinearity='relu'\n",
    "        )\n",
    "    \n",
    "    def create_embedding_nets(self):\n",
    "        # marker_dim is passed. timeseries_dim is 2\n",
    "        self.marker_embedding_net = nn.Linear(self.marker_dim, self.marker_embed_dim)\n",
    "        \n",
    "        self.time_embedding_net = nn.Sequential(\n",
    "            nn.Linear(2, self.time_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.time_embed_dim, self.time_embed_dim)\n",
    "        )\n",
    "        \n",
    "    def create_marker_generation_net(self):\n",
    "        \"\"\"\n",
    "            Generate network to create marker sufficient statistics using\n",
    "            rnn's hidden layer\n",
    "        \"\"\"\n",
    "        self.marker_gen_hidden = nn.Sequential(\n",
    "            nn.Linear(self.hidden_layer_dim, self.marker_shared_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.generated_marker_mu = nn.Linear(self.marker_shared_dim, self.marker_dim)\n",
    "        self.generated_marker_logvar = nn.Sequential(\n",
    "            nn.Linear(self.marker_shared_dim, self.marker_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "    \n",
    "    def create_time_likelihood_net(self):\n",
    "        self.h_influence = nn.Linear(self.hidden_layer_dim, 1, bias=False)\n",
    "        self.base_intensity = nn.Parameter(torch.zeros(1,1,1))\n",
    "        self.wt = nn.Parameter(torch.ones(1,1,1))\n",
    "    \n",
    "    \n",
    "    ############ METHODS #############\n",
    "    \n",
    "    def _embed_data(self, marker_seq, time_seq):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            marker_seq: Tensor of shape TxBSx marker_dim\n",
    "            time_seq: Tensor of shape TxBSx 2\n",
    "            Output:\n",
    "            marker_seq_emb: Tensor of shape T x BS x marker_embed_dim\n",
    "            time_seq_emb: Tensor of shape T x BS x time_embed_dim\n",
    "        \"\"\"\n",
    "        marker_seq_emb = self.marker_embedding_net(marker_seq)\n",
    "        time_seq_emb = self.time_embedding_net(time_seq)\n",
    "        return marker_seq_emb, time_seq_emb\n",
    "    \n",
    "    \n",
    "    \n",
    "    def marker_log_likelihood(self, h_seq, marker_seq):\n",
    "        \"\"\"\n",
    "        Use the h_seq to generate the distribution for the markers,\n",
    "        and use that distribution to compute the log likelihood of the marker_seq\n",
    "            Input:  \n",
    "                    h_seq   : Tensor of shape T x BS x hidden_layer_dim (if real)\n",
    "                    marker_seq : Tensor of shape T x BS x marker_dim\n",
    "            Output:\n",
    "                    log_likelihood_marker_seq : T x BS x marker_dim\n",
    "        \"\"\"\n",
    "        marker_gen_shared = self.marker_gen_hidden(h_seq)\n",
    "        mu, logvar = self.generated_marker_mu(marker_gen_shared), self.generated_marker_logvar(marker_gen_shared)\n",
    "        \n",
    "        marker_gen_dist = Normal(mu, logvar.exp().sqrt())\n",
    "        log_likelihood_marker_seq = marker_gen_dist.log_prob(marker_seq)\n",
    "        \n",
    "        return log_likelihood_marker_seq\n",
    "        \n",
    "        \n",
    "    def time_log_likelihood(self, h_seq, time_seq):\n",
    "        \"\"\"\n",
    "        Use the h_seq to compute the log likelihood of the time_seq\n",
    "        using the formula in the paper\n",
    "            Input:  \n",
    "                    h_seq   : Tensor of shape T x BS x hidden_layer_dim (if real)\n",
    "                    time_seq : Tensor of shape T x BS x 2 . Last dimension is [times, intervals]\n",
    "            Output:\n",
    "                    log_likelihood_time_seq : T x BS x 1\n",
    "        \"\"\"\n",
    "        past_influence = self.h_influence(h_seq)\n",
    "        current_influence = self.wt * time_seq[:,:,1:2]\n",
    "        base_intensity = self.base_intensity\n",
    "        \n",
    "        term1 = past_influence + current_influence + base_intensity\n",
    "        term2 = past_influence + base_intensity\n",
    "        \n",
    "        # After factorizing the formula in the paper\n",
    "        log_likelihood_time_seq = term1 + (term2.exp() - term1.exp())/self.wt\n",
    "        \n",
    "        return log_likelihood_time_seq\n",
    "    \n",
    "    def forward(self, marker_seq, time_seq):\n",
    "        # Transform markers and timesteps into the embedding spaces\n",
    "        marker_seq_emb, time_seq_emb = self._embed_data(marker_seq, time_seq)\n",
    "        T,BS,_ = marker_seq_emb.shape\n",
    "        \n",
    "        # Run RNN over the concatenated sequence [marker_seq_emb, time_seq_emb]\n",
    "        time_marker_combined = torch.cat([marker_seq_emb, time_seq_emb], dim=-1)\n",
    "        h_0 = torch.zeros(1, BS, self.hidden_layer_dim).to(device)\n",
    "        hidden_seq, _ = self.rnn(time_marker_combined, h_0)\n",
    "        \n",
    "        # compute the marker and time log likelihoods\n",
    "        marker_ll = self.marker_log_likelihood(hidden_seq, marker_seq)\n",
    "        time_ll = self.time_log_likelihood(hidden_seq, time_seq)\n",
    "        \n",
    "        likelihood_loss = marker_ll.sum() + time_ll.sum()\n",
    "        NLL = -likelihood_loss\n",
    "        return NLL, [-marker_ll.sum().item(), -time_ll.sum().item() ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _data, _ = generate_mpp(num_sample=200)\n",
    "# model = RMTPP().to(device)\n",
    "# model(_data['x'], _data['t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ import generate_mpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import train\n",
    "from rmtpp import rmtpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data = None, val_data=None, lr= 1e-3, epoch = 500, batch_size = 64):\n",
    "    if data == None:\n",
    "        data, val_data = generate_mpp()\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch_number in range(epoch):\n",
    "        train(model, epoch_number, data, optimizer, batch_size, val_data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, data, val_data):\n",
    "    model = model().to(device)\n",
    "#     data, _ = generate_mpp(type='hawkes', num_sample=1000)\n",
    "#     val_data, _ = generate_mpp(type='hawkes', num_sample = 200)\n",
    "    print(\"Times: Data Shape: {}, Val Data Shape: {}\".format(data['t'].shape, val_data['t'].shape))\n",
    "    print(\"Markers: Data Shape: {}, Val Data Shape: {}\".format(data['x'].shape, val_data['x'].shape))\n",
    "    trainer(model, data=data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times: Data Shape: torch.Size([100, 1000, 2]), Val Data Shape: torch.Size([100, 200, 2])\n",
      "Markers: Data Shape: torch.Size([100, 1000, 20]), Val Data Shape: torch.Size([100, 200, 20])\n",
      "Epoch: 0, NLL Loss: 18633445881.36, Val Loss: 77549176.0, Time took: 1.2529633045196533\n",
      "Train loss Meta Info:  [114.342859375, 73973.088]\n",
      "Val Loss Meta Info:  [5703.70625, 77543480.32]\n",
      "\n",
      "Epoch: 1, NLL Loss: 17090740647.76, Val Loss: 71676648.0, Time took: 1.2832868099212646\n",
      "Train loss Meta Info:  [114.31271875, 69776.216]\n",
      "Val Loss Meta Info:  [5702.495, 71670958.08]\n",
      "\n",
      "Epoch: 2, NLL Loss: 15706279251.156, Val Loss: 66108732.0, Time took: 1.1665372848510742\n",
      "Train loss Meta Info:  [114.287203125, 65687.616]\n",
      "Val Loss Meta Info:  [5700.2225, 66103040.0]\n",
      "\n",
      "Epoch: 3, NLL Loss: 14370354843.888, Val Loss: 60820172.0, Time took: 1.4386906623840332\n",
      "Train loss Meta Info:  [114.240140625, 61854.432]\n",
      "Val Loss Meta Info:  [5698.41625, 60814484.48]\n",
      "\n",
      "Epoch: 4, NLL Loss: 13163647470.644, Val Loss: 55654144.0, Time took: 1.519726276397705\n",
      "Train loss Meta Info:  [114.2041640625, 58185.464]\n",
      "Val Loss Meta Info:  [5697.485625, 55648460.8]\n",
      "\n",
      "Epoch: 5, NLL Loss: 11846606056.544, Val Loss: 50928864.0, Time took: 1.4927778244018555\n",
      "Train loss Meta Info:  [114.18534375, 54548.848]\n",
      "Val Loss Meta Info:  [5697.01125, 50923176.96]\n",
      "\n",
      "Epoch: 6, NLL Loss: 10528407146.222, Val Loss: 46662484.0, Time took: 1.47483491897583\n",
      "Train loss Meta Info:  [114.1715625, 51015.804]\n",
      "Val Loss Meta Info:  [5696.51125, 46656788.48]\n",
      "\n",
      "Epoch: 7, NLL Loss: 9551577485.954, Val Loss: 42785772.0, Time took: 1.4440782070159912\n",
      "Train loss Meta Info:  [114.1564375, 47699.656]\n",
      "Val Loss Meta Info:  [5695.88875, 42780083.2]\n",
      "\n",
      "Epoch: 8, NLL Loss: 8667726158.744, Val Loss: 39163300.0, Time took: 1.412257432937622\n",
      "Train loss Meta Info:  [114.137953125, 44483.156]\n",
      "Val Loss Meta Info:  [5695.27125, 39157614.08]\n",
      "\n",
      "Epoch: 9, NLL Loss: 7855084763.938, Val Loss: 35810548.0, Time took: 1.4902853965759277\n",
      "Train loss Meta Info:  [114.118984375, 41440.8]\n",
      "Val Loss Meta Info:  [5694.729375, 35804853.76]\n",
      "\n",
      "Epoch: 10, NLL Loss: 7109428925.666, Val Loss: 32702738.0, Time took: 1.408381700515747\n",
      "Train loss Meta Info:  [114.101828125, 38406.924]\n",
      "Val Loss Meta Info:  [5694.278125, 32697044.48]\n",
      "\n",
      "Epoch: 11, NLL Loss: 6416617132.924, Val Loss: 29826078.0, Time took: 1.2647900581359863\n",
      "Train loss Meta Info:  [114.085703125, 35373.564]\n",
      "Val Loss Meta Info:  [5693.86, 29820382.72]\n",
      "\n",
      "Epoch: 12, NLL Loss: 5767499722.896, Val Loss: 27204280.0, Time took: 1.536982774734497\n",
      "Train loss Meta Info:  [114.070015625, 32502.978]\n",
      "Val Loss Meta Info:  [5693.36625, 27198586.88]\n",
      "\n",
      "Epoch: 13, NLL Loss: 5130823925.75, Val Loss: 24899730.0, Time took: 1.5138070583343506\n",
      "Train loss Meta Info:  [114.0528828125, 29854.746]\n",
      "Val Loss Meta Info:  [5692.59625, 24894039.04]\n",
      "\n",
      "Epoch: 14, NLL Loss: 4544529408.893, Val Loss: 22869902.0, Time took: 1.482370138168335\n",
      "Train loss Meta Info:  [114.0296875, 27367.838]\n",
      "Val Loss Meta Info:  [5691.51125, 22864212.48]\n",
      "\n",
      "Epoch: 15, NLL Loss: 4032650870.811, Val Loss: 20874132.0, Time took: 1.6350061893463135\n",
      "Train loss Meta Info:  [114.00009375, 25031.408]\n",
      "Val Loss Meta Info:  [5690.269375, 20868440.32]\n",
      "\n",
      "Epoch: 16, NLL Loss: 3597637451.597, Val Loss: 18977084.0, Time took: 1.4106190204620361\n",
      "Train loss Meta Info:  [113.967265625, 22810.908]\n",
      "Val Loss Meta Info:  [5688.93875, 18971395.84]\n",
      "\n",
      "Epoch: 17, NLL Loss: 3199659812.684, Val Loss: 17274810.0, Time took: 1.717578411102295\n",
      "Train loss Meta Info:  [113.9317421875, 20675.164]\n",
      "Val Loss Meta Info:  [5687.64125, 17269121.28]\n",
      "\n",
      "Epoch: 18, NLL Loss: 2844324519.103, Val Loss: 15720380.0, Time took: 1.6189916133880615\n",
      "Train loss Meta Info:  [113.896171875, 18665.54]\n",
      "Val Loss Meta Info:  [5686.39125, 15714693.12]\n",
      "\n",
      "Epoch: 19, NLL Loss: 2507509364.284, Val Loss: 14271979.0, Time took: 1.6394524574279785\n",
      "Train loss Meta Info:  [113.860984375, 16785.84]\n",
      "Val Loss Meta Info:  [5685.255, 14266295.04]\n",
      "\n",
      "Epoch: 20, NLL Loss: 2206501109.557, Val Loss: 12717575.0, Time took: 1.5669317245483398\n",
      "Train loss Meta Info:  [113.829453125, 15057.614]\n",
      "Val Loss Meta Info:  [5684.429375, 12711891.2]\n",
      "\n",
      "Epoch: 21, NLL Loss: 1934524928.9585, Val Loss: 11218853.0, Time took: 1.4542341232299805\n",
      "Train loss Meta Info:  [113.805171875, 13430.324]\n",
      "Val Loss Meta Info:  [5683.776875, 11213167.36]\n",
      "\n",
      "Epoch: 22, NLL Loss: 1687637952.617, Val Loss: 9874861.0, Time took: 1.4645962715148926\n",
      "Train loss Meta Info:  [113.7860703125, 11915.402]\n",
      "Val Loss Meta Info:  [5683.26375, 9869176.96]\n",
      "\n",
      "Epoch: 23, NLL Loss: 1471180683.227, Val Loss: 8680326.0, Time took: 1.2509887218475342\n",
      "Train loss Meta Info:  [113.7722890625, 10514.737]\n",
      "Val Loss Meta Info:  [5683.0325, 8674642.56]\n",
      "\n",
      "Epoch: 24, NLL Loss: 1281063428.959, Val Loss: 7614128.0, Time took: 1.096738338470459\n",
      "Train loss Meta Info:  [113.7665625, 9248.198]\n",
      "Val Loss Meta Info:  [5683.1525, 7608443.52]\n",
      "\n",
      "Epoch: 25, NLL Loss: 1113733004.7835, Val Loss: 6665657.5, Time took: 1.5138943195343018\n",
      "Train loss Meta Info:  [113.769546875, 8112.999]\n",
      "Val Loss Meta Info:  [5683.393125, 6659972.48]\n",
      "\n",
      "Epoch: 26, NLL Loss: 966294843.584, Val Loss: 5827936.5, Time took: 1.5658159255981445\n",
      "Train loss Meta Info:  [113.776734375, 7103.494]\n",
      "Val Loss Meta Info:  [5683.4975, 5822252.16]\n",
      "\n",
      "Epoch: 27, NLL Loss: 837434755.127, Val Loss: 5091777.5, Time took: 1.4398303031921387\n",
      "Train loss Meta Info:  [113.7813125, 6206.483]\n",
      "Val Loss Meta Info:  [5683.20125, 5086092.8]\n",
      "\n",
      "Epoch: 28, NLL Loss: 726045970.293, Val Loss: 4444337.0, Time took: 1.1452224254608154\n",
      "Train loss Meta Info:  [113.77753125, 5412.339]\n",
      "Val Loss Meta Info:  [5682.43, 4438654.72]\n",
      "\n",
      "Epoch: 29, NLL Loss: 629355629.50625, Val Loss: 3876352.25, Time took: 1.5759053230285645\n",
      "Train loss Meta Info:  [113.764125, 4709.0745]\n",
      "Val Loss Meta Info:  [5681.360625, 3870671.04]\n",
      "\n",
      "Epoch: 30, NLL Loss: 545624552.86625, Val Loss: 3378099.0, Time took: 1.571045160293579\n",
      "Train loss Meta Info:  [113.7450078125, 4090.42925]\n",
      "Val Loss Meta Info:  [5680.238125, 3372419.2]\n",
      "\n",
      "Epoch: 31, NLL Loss: 470296862.352, Val Loss: 2947658.5, Time took: 1.1910841464996338\n",
      "Train loss Meta Info:  [113.725671875, 3551.563]\n",
      "Val Loss Meta Info:  [5679.161875, 2941979.84]\n",
      "\n",
      "Epoch: 32, NLL Loss: 405649019.535, Val Loss: 2566562.0, Time took: 1.3923437595367432\n",
      "Train loss Meta Info:  [113.70909375, 3093.15775]\n",
      "Val Loss Meta Info:  [5678.185625, 2560884.48]\n",
      "\n",
      "Epoch: 33, NLL Loss: 349620631.857, Val Loss: 2231690.0, Time took: 1.5351760387420654\n",
      "Train loss Meta Info:  [113.696640625, 2689.94225]\n",
      "Val Loss Meta Info:  [5677.28375, 2226012.8]\n",
      "\n",
      "Epoch: 34, NLL Loss: 301390466.7025, Val Loss: 1940324.375, Time took: 1.5471115112304688\n",
      "Train loss Meta Info:  [113.687, 2335.673]\n",
      "Val Loss Meta Info:  [5676.47875, 1934647.84]\n",
      "\n",
      "Epoch: 35, NLL Loss: 259061292.364125, Val Loss: 1690821.125, Time took: 1.4988949298858643\n",
      "Train loss Meta Info:  [113.67903125, 2026.327375]\n",
      "Val Loss Meta Info:  [5675.886875, 1685145.28]\n",
      "\n",
      "Epoch: 36, NLL Loss: 223134086.292125, Val Loss: 1474278.5, Time took: 1.264354944229126\n",
      "Train loss Meta Info:  [113.6743984375, 1762.410875]\n",
      "Val Loss Meta Info:  [5675.43625, 1468603.36]\n",
      "\n",
      "Epoch: 37, NLL Loss: 191867414.72575, Val Loss: 1287198.375, Time took: 1.2118782997131348\n",
      "Train loss Meta Info:  [113.67021875, 1532.854375]\n",
      "Val Loss Meta Info:  [5675.1875, 1281523.28]\n",
      "\n",
      "Epoch: 38, NLL Loss: 165352523.8165, Val Loss: 1125075.0, Time took: 1.287412166595459\n",
      "Train loss Meta Info:  [113.6668203125, 1336.0435]\n",
      "Val Loss Meta Info:  [5675.180625, 1119399.84]\n",
      "\n",
      "Epoch: 39, NLL Loss: 142713541.6505, Val Loss: 983968.4375, Time took: 1.3114733695983887\n",
      "Train loss Meta Info:  [113.66525, 1165.71075]\n",
      "Val Loss Meta Info:  [5675.38875, 978293.36]\n",
      "\n",
      "Epoch: 40, NLL Loss: 123423063.709375, Val Loss: 862412.4375, Time took: 1.1324641704559326\n",
      "Train loss Meta Info:  [113.6649765625, 1018.766625]\n",
      "Val Loss Meta Info:  [5675.69375, 856736.72]\n",
      "\n",
      "Epoch: 41, NLL Loss: 107000070.8674375, Val Loss: 757583.125, Time took: 1.1158239841461182\n",
      "Train loss Meta Info:  [113.663625, 891.914125]\n",
      "Val Loss Meta Info:  [5675.8575, 751907.28]\n",
      "\n",
      "Epoch: 42, NLL Loss: 92638613.3610625, Val Loss: 667580.8125, Time took: 1.3473713397979736\n",
      "Train loss Meta Info:  [113.65915625, 782.442125]\n",
      "Val Loss Meta Info:  [5675.6925, 661905.2]\n",
      "\n",
      "Epoch: 43, NLL Loss: 80503823.9120625, Val Loss: 589475.5625, Time took: 1.3635036945343018\n",
      "Train loss Meta Info:  [113.649578125, 689.80775]\n",
      "Val Loss Meta Info:  [5675.22375, 583800.4]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, NLL Loss: 70089975.363125, Val Loss: 521759.28125, Time took: 1.000394344329834\n",
      "Train loss Meta Info:  [113.634453125, 609.01625]\n",
      "Val Loss Meta Info:  [5674.523125, 516084.8]\n",
      "\n",
      "Epoch: 45, NLL Loss: 61167920.16453125, Val Loss: 462509.1875, Time took: 1.2740373611450195\n",
      "Train loss Meta Info:  [113.6175703125, 538.796625]\n",
      "Val Loss Meta Info:  [5673.7925, 456835.44]\n",
      "\n",
      "Epoch: 46, NLL Loss: 53525516.587875, Val Loss: 410513.4375, Time took: 1.3330769538879395\n",
      "Train loss Meta Info:  [113.6046015625, 477.9215]\n",
      "Val Loss Meta Info:  [5673.340625, 404840.0]\n",
      "\n",
      "Epoch: 47, NLL Loss: 46973814.953875, Val Loss: 365185.5625, Time took: 1.2974839210510254\n",
      "Train loss Meta Info:  [113.6016875, 425.1186875]\n",
      "Val Loss Meta Info:  [5673.30375, 359512.16]\n",
      "\n",
      "Epoch: 48, NLL Loss: 41349963.09453125, Val Loss: 325865.8125, Time took: 1.5790579319000244\n",
      "Train loss Meta Info:  [113.60975, 379.3118125]\n",
      "Val Loss Meta Info:  [5673.5875, 320192.22]\n",
      "\n",
      "Epoch: 49, NLL Loss: 36506859.68409375, Val Loss: 291748.84375, Time took: 1.6248862743377686\n",
      "Train loss Meta Info:  [113.622953125, 339.5555]\n",
      "Val Loss Meta Info:  [5673.83, 286075.04]\n",
      "\n",
      "Epoch: 50, NLL Loss: 32284001.6385, Val Loss: 262135.171875, Time took: 1.4231534004211426\n",
      "Train loss Meta Info:  [113.6322109375, 305.02259375]\n",
      "Val Loss Meta Info:  [5673.769375, 256461.38]\n",
      "\n",
      "Epoch: 51, NLL Loss: 28547876.3289375, Val Loss: 236213.375, Time took: 1.6397418975830078\n",
      "Train loss Meta Info:  [113.628796875, 274.4111875]\n",
      "Val Loss Meta Info:  [5673.434375, 230539.96]\n",
      "\n",
      "Epoch: 52, NLL Loss: 25300066.616, Val Loss: 213418.8125, Time took: 1.503265380859375\n",
      "Train loss Meta Info:  [113.6165625, 247.220328125]\n",
      "Val Loss Meta Info:  [5673.005, 207745.84]\n",
      "\n",
      "Epoch: 53, NLL Loss: 22479608.31215625, Val Loss: 193449.296875, Time took: 1.1136829853057861\n",
      "Train loss Meta Info:  [113.603421875, 223.668390625]\n",
      "Val Loss Meta Info:  [5672.7475, 187776.54]\n",
      "\n",
      "Epoch: 54, NLL Loss: 20027289.842875, Val Loss: 175719.796875, Time took: 1.3517191410064697\n",
      "Train loss Meta Info:  [113.595203125, 202.99015625]\n",
      "Val Loss Meta Info:  [5672.72875, 170047.08]\n",
      "\n",
      "Epoch: 55, NLL Loss: 17892093.700875, Val Loss: 159938.875, Time took: 1.7226791381835938\n",
      "Train loss Meta Info:  [113.5949140625, 184.61728125]\n",
      "Val Loss Meta Info:  [5672.8575, 154266.02]\n",
      "\n",
      "Epoch: 56, NLL Loss: 16030068.36184375, Val Loss: 145951.5, Time took: 1.3982763290405273\n",
      "Train loss Meta Info:  [113.596921875, 168.25834375]\n",
      "Val Loss Meta Info:  [5672.861875, 140278.63]\n",
      "\n",
      "Epoch: 57, NLL Loss: 14403225.62715625, Val Loss: 133607.640625, Time took: 1.2756540775299072\n",
      "Train loss Meta Info:  [113.5953828125, 153.8673125]\n",
      "Val Loss Meta Info:  [5672.675, 127934.95]\n",
      "\n",
      "Epoch: 58, NLL Loss: 12978955.592546875, Val Loss: 122577.6171875, Time took: 1.1688587665557861\n",
      "Train loss Meta Info:  [113.590234375, 141.196765625]\n",
      "Val Loss Meta Info:  [5672.52125, 116905.09]\n",
      "\n",
      "Epoch: 59, NLL Loss: 11729408.595953126, Val Loss: 112729.3046875, Time took: 1.2159757614135742\n",
      "Train loss Meta Info:  [113.5863515625, 130.027671875]\n",
      "Val Loss Meta Info:  [5672.38875, 107056.91]\n",
      "\n",
      "Epoch: 60, NLL Loss: 10630846.40653125, Val Loss: 103930.1015625, Time took: 1.3040070533752441\n",
      "Train loss Meta Info:  [113.5875546875, 120.16975]\n",
      "Val Loss Meta Info:  [5672.32625, 98257.77]\n",
      "\n",
      "Epoch: 61, NLL Loss: 9662904.708625, Val Loss: 95835.9453125, Time took: 1.4857890605926514\n",
      "Train loss Meta Info:  [113.591578125, 111.465453125]\n",
      "Val Loss Meta Info:  [5672.3325, 90163.61]\n",
      "\n",
      "Epoch: 62, NLL Loss: 8808042.99015625, Val Loss: 88552.78125, Time took: 1.2368495464324951\n",
      "Train loss Meta Info:  [113.598234375, 103.7535625]\n",
      "Val Loss Meta Info:  [5672.2375, 82880.55]\n",
      "\n",
      "Epoch: 63, NLL Loss: 8051302.59128125, Val Loss: 82060.125, Time took: 1.2976787090301514\n",
      "Train loss Meta Info:  [113.5988359375, 96.873703125]\n",
      "Val Loss Meta Info:  [5672.10375, 76388.025]\n",
      "\n",
      "Epoch: 64, NLL Loss: 7367711.09990625, Val Loss: 76540.2109375, Time took: 1.2889456748962402\n",
      "Train loss Meta Info:  [113.5947890625, 90.804421875]\n",
      "Val Loss Meta Info:  [5671.975, 70868.235]\n",
      "\n",
      "Epoch: 65, NLL Loss: 6742017.787703125, Val Loss: 71507.9296875, Time took: 1.2178587913513184\n",
      "Train loss Meta Info:  [113.587828125, 85.710765625]\n",
      "Val Loss Meta Info:  [5671.95125, 65835.985]\n",
      "\n",
      "Epoch: 66, NLL Loss: 6175465.0756875, Val Loss: 66935.578125, Time took: 1.2092390060424805\n",
      "Train loss Meta Info:  [113.582296875, 81.101578125]\n",
      "Val Loss Meta Info:  [5671.98625, 61263.6]\n",
      "\n",
      "Epoch: 67, NLL Loss: 5664381.3108125, Val Loss: 62785.7890625, Time took: 1.327531099319458\n",
      "Train loss Meta Info:  [113.58084375, 76.9431875]\n",
      "Val Loss Meta Info:  [5672.051875, 57113.735]\n",
      "\n",
      "Epoch: 68, NLL Loss: 5203758.3069375, Val Loss: 59019.68359375, Time took: 1.2538282871246338\n",
      "Train loss Meta Info:  [113.5795703125, 73.196671875]\n",
      "Val Loss Meta Info:  [5672.1175, 53347.58]\n",
      "\n",
      "Epoch: 69, NLL Loss: 4788607.70521875, Val Loss: 55600.1484375, Time took: 1.2850456237792969\n",
      "Train loss Meta Info:  [113.578859375, 69.82325]\n",
      "Val Loss Meta Info:  [5672.0375, 49928.115]\n",
      "\n",
      "Epoch: 70, NLL Loss: 4414192.921765625, Val Loss: 52493.0, Time took: 1.3189773559570312\n",
      "Train loss Meta Info:  [113.5770234375, 66.785796875]\n",
      "Val Loss Meta Info:  [5671.92125, 46821.08]\n",
      "\n",
      "Epoch: 71, NLL Loss: 4076204.6776875, Val Loss: 49666.91015625, Time took: 1.4185373783111572\n",
      "Train loss Meta Info:  [113.576421875, 64.0502265625]\n",
      "Val Loss Meta Info:  [5671.80625, 43995.1]\n",
      "\n",
      "Epoch: 72, NLL Loss: 3770744.114828125, Val Loss: 47093.71875, Time took: 1.430823802947998\n",
      "Train loss Meta Info:  [113.579890625, 61.58530078125]\n",
      "Val Loss Meta Info:  [5671.7475, 41421.9725]\n",
      "\n",
      "Epoch: 73, NLL Loss: 3494338.41865625, Val Loss: 44748.04296875, Time took: 1.3272161483764648\n",
      "Train loss Meta Info:  [113.582828125, 59.36309375]\n",
      "Val Loss Meta Info:  [5671.735625, 39076.31]\n",
      "\n",
      "Epoch: 74, NLL Loss: 3243873.66909375, Val Loss: 42606.94921875, Time took: 1.30739426612854\n",
      "Train loss Meta Info:  [113.584546875, 57.3584140625]\n",
      "Val Loss Meta Info:  [5671.708125, 36935.2425]\n",
      "\n",
      "Epoch: 75, NLL Loss: 3016607.39128125, Val Loss: 40650.1796875, Time took: 1.3178694248199463\n",
      "Train loss Meta Info:  [113.584, 55.54858984375]\n",
      "Val Loss Meta Info:  [5671.688125, 34978.495]\n",
      "\n",
      "Epoch: 76, NLL Loss: 2810098.845671875, Val Loss: 38859.41015625, Time took: 1.3653035163879395\n",
      "Train loss Meta Info:  [113.579859375, 53.91365625]\n",
      "Val Loss Meta Info:  [5671.605, 33187.8075]\n",
      "\n",
      "Epoch: 77, NLL Loss: 2622164.723296875, Val Loss: 37218.4765625, Time took: 1.6734726428985596\n",
      "Train loss Meta Info:  [113.5746953125, 52.435515625]\n",
      "Val Loss Meta Info:  [5671.573125, 31546.9]\n",
      "\n",
      "Epoch: 78, NLL Loss: 2450889.613890625, Val Loss: 35711.91015625, Time took: 1.3463940620422363\n",
      "Train loss Meta Info:  [113.571921875, 51.09812890625]\n",
      "Val Loss Meta Info:  [5671.629375, 30040.28]\n",
      "\n",
      "Epoch: 79, NLL Loss: 2294562.752546875, Val Loss: 34327.2734375, Time took: 1.5556747913360596\n",
      "Train loss Meta Info:  [113.5738125, 49.887140625]\n",
      "Val Loss Meta Info:  [5671.681875, 28655.5925]\n",
      "\n",
      "Epoch: 80, NLL Loss: 2151670.455203125, Val Loss: 33053.265625, Time took: 1.5224053859710693\n",
      "Train loss Meta Info:  [113.5749375, 48.78983984375]\n",
      "Val Loss Meta Info:  [5671.71875, 27381.545]\n",
      "\n",
      "Epoch: 81, NLL Loss: 2020854.705828125, Val Loss: 31879.349609375, Time took: 1.471282958984375\n",
      "Train loss Meta Info:  [113.5747890625, 47.79480859375]\n",
      "Val Loss Meta Info:  [5671.628125, 26207.7225]\n",
      "\n",
      "Epoch: 82, NLL Loss: 1900940.97465625, Val Loss: 30796.380859375, Time took: 1.1896989345550537\n",
      "Train loss Meta Info:  [113.5719921875, 46.891953125]\n",
      "Val Loss Meta Info:  [5671.575, 25124.8075]\n",
      "\n",
      "Epoch: 83, NLL Loss: 1790836.096828125, Val Loss: 29795.8515625, Time took: 1.4170899391174316\n",
      "Train loss Meta Info:  [113.573671875, 46.0721171875]\n",
      "Val Loss Meta Info:  [5671.51375, 24124.3425]\n",
      "\n",
      "Epoch: 84, NLL Loss: 1689610.395375, Val Loss: 28870.41015625, Time took: 1.5604925155639648\n",
      "Train loss Meta Info:  [113.5748046875, 45.3271796875]\n",
      "Val Loss Meta Info:  [5671.4575, 23198.955]\n",
      "\n",
      "Epoch: 85, NLL Loss: 1596403.814109375, Val Loss: 28013.27734375, Time took: 1.2832715511322021\n",
      "Train loss Meta Info:  [113.5746953125, 44.64991796875]\n",
      "Val Loss Meta Info:  [5671.45875, 22341.82]\n",
      "\n",
      "Epoch: 86, NLL Loss: 1510459.855421875, Val Loss: 27218.326171875, Time took: 0.9783673286437988\n",
      "Train loss Meta Info:  [113.5752265625, 44.033765625]\n",
      "Val Loss Meta Info:  [5671.44875, 21546.8775]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87, NLL Loss: 1431104.223234375, Val Loss: 26480.173828125, Time took: 0.7957284450531006\n",
      "Train loss Meta Info:  [113.573703125, 43.47289453125]\n",
      "Val Loss Meta Info:  [5671.495625, 20808.68]\n",
      "\n",
      "Epoch: 88, NLL Loss: 1357731.061875, Val Loss: 25793.78515625, Time took: 0.7976751327514648\n",
      "Train loss Meta Info:  [113.573046875, 42.96184375]\n",
      "Val Loss Meta Info:  [5671.48625, 20122.3025]\n",
      "\n",
      "Epoch: 89, NLL Loss: 1289783.8505625, Val Loss: 25154.779296875, Time took: 0.8050529956817627\n",
      "Train loss Meta Info:  [113.5711796875, 42.49659375]\n",
      "Val Loss Meta Info:  [5671.49125, 19483.29]\n",
      "\n",
      "Epoch: 90, NLL Loss: 1226789.1778125, Val Loss: 24559.1015625, Time took: 0.7849588394165039\n",
      "Train loss Meta Info:  [113.570640625, 42.07246484375]\n",
      "Val Loss Meta Info:  [5671.466875, 18887.635]\n",
      "\n",
      "Epoch: 91, NLL Loss: 1168302.4654375, Val Loss: 24003.173828125, Time took: 1.0338473320007324\n",
      "Train loss Meta Info:  [113.5697578125, 41.68563671875]\n",
      "Val Loss Meta Info:  [5671.458125, 18331.715]\n",
      "\n",
      "Epoch: 92, NLL Loss: 1113930.38165625, Val Loss: 23483.671875, Time took: 1.0552752017974854\n",
      "Train loss Meta Info:  [113.5708984375, 41.332734375]\n",
      "Val Loss Meta Info:  [5671.4225, 17812.2525]\n",
      "\n",
      "Epoch: 93, NLL Loss: 1063314.740796875, Val Loss: 22997.609375, Time took: 1.128711223602295\n",
      "Train loss Meta Info:  [113.571484375, 41.01071484375]\n",
      "Val Loss Meta Info:  [5671.366875, 17326.2425]\n",
      "\n",
      "Epoch: 94, NLL Loss: 1016133.00384375, Val Loss: 22542.384765625, Time took: 1.1469273567199707\n",
      "Train loss Meta Info:  [113.5712578125, 40.7167734375]\n",
      "Val Loss Meta Info:  [5671.3775, 16871.00625]\n",
      "\n",
      "Epoch: 95, NLL Loss: 972101.975109375, Val Loss: 22115.47265625, Time took: 1.150526523590088\n",
      "Train loss Meta Info:  [113.571875, 40.4484453125]\n",
      "Val Loss Meta Info:  [5671.369375, 16444.1025]\n",
      "\n",
      "Epoch: 96, NLL Loss: 930955.963375, Val Loss: 21714.669921875, Time took: 1.1489205360412598\n",
      "Train loss Meta Info:  [113.5700546875, 40.203453125]\n",
      "Val Loss Meta Info:  [5671.40875, 16043.26125]\n",
      "\n",
      "Epoch: 97, NLL Loss: 892459.0054375, Val Loss: 21337.888671875, Time took: 0.9946134090423584\n",
      "Train loss Meta Info:  [113.5703125, 39.9797734375]\n",
      "Val Loss Meta Info:  [5671.388125, 15666.49875]\n",
      "\n",
      "Epoch: 98, NLL Loss: 856393.878015625, Val Loss: 20983.349609375, Time took: 1.226811170578003\n",
      "Train loss Meta Info:  [113.5697421875, 39.7755625]\n",
      "Val Loss Meta Info:  [5671.381875, 15311.9675]\n",
      "\n",
      "Epoch: 99, NLL Loss: 822570.701421875, Val Loss: 20649.357421875, Time took: 1.1252946853637695\n",
      "Train loss Meta Info:  [113.5690703125, 39.5891484375]\n",
      "Val Loss Meta Info:  [5671.364375, 14977.99125]\n",
      "\n",
      "Epoch: 100, NLL Loss: 790812.003890625, Val Loss: 20334.326171875, Time took: 1.1426289081573486\n",
      "Train loss Meta Info:  [113.5701875, 39.41903125]\n",
      "Val Loss Meta Info:  [5671.31375, 14663.0125]\n",
      "\n",
      "Epoch: 101, NLL Loss: 760955.78675, Val Loss: 20036.96875, Time took: 1.3023414611816406\n",
      "Train loss Meta Info:  [113.56934375, 39.263828125]\n",
      "Val Loss Meta Info:  [5671.33625, 14365.63375]\n",
      "\n",
      "Epoch: 102, NLL Loss: 732855.772078125, Val Loss: 19755.90234375, Time took: 1.30739164352417\n",
      "Train loss Meta Info:  [113.5701875, 39.122296875]\n",
      "Val Loss Meta Info:  [5671.30875, 14084.595]\n",
      "\n",
      "Epoch: 103, NLL Loss: 706380.74934375, Val Loss: 19490.052734375, Time took: 1.2834539413452148\n",
      "Train loss Meta Info:  [113.568078125, 38.99331640625]\n",
      "Val Loss Meta Info:  [5671.36875, 13818.68375]\n",
      "\n",
      "Epoch: 104, NLL Loss: 681406.81390625, Val Loss: 19238.181640625, Time took: 1.2324154376983643\n",
      "Train loss Meta Info:  [113.568828125, 38.87583984375]\n",
      "Val Loss Meta Info:  [5671.3175, 13566.86375]\n",
      "\n",
      "Epoch: 105, NLL Loss: 657826.1371875, Val Loss: 18999.474609375, Time took: 1.2146165370941162\n",
      "Train loss Meta Info:  [113.5673984375, 38.7689765625]\n",
      "Val Loss Meta Info:  [5671.3325, 13328.1425]\n",
      "\n",
      "Epoch: 106, NLL Loss: 635537.8210625, Val Loss: 18772.888671875, Time took: 1.1602702140808105\n",
      "Train loss Meta Info:  [113.5688984375, 38.6718359375]\n",
      "Val Loss Meta Info:  [5671.286875, 13101.60125]\n",
      "\n",
      "Epoch: 107, NLL Loss: 614446.244609375, Val Loss: 18557.712890625, Time took: 0.8593711853027344\n",
      "Train loss Meta Info:  [113.5695625, 38.5836640625]\n",
      "Val Loss Meta Info:  [5671.28875, 12886.425]\n",
      "\n",
      "Epoch: 108, NLL Loss: 594468.57634375, Val Loss: 18353.158203125, Time took: 1.2371153831481934\n",
      "Train loss Meta Info:  [113.56896875, 38.5037421875]\n",
      "Val Loss Meta Info:  [5671.305, 12681.855]\n",
      "\n",
      "Epoch: 109, NLL Loss: 575529.37290625, Val Loss: 18158.431640625, Time took: 1.2141647338867188\n",
      "Train loss Meta Info:  [113.5690390625, 38.43146484375]\n",
      "Val Loss Meta Info:  [5671.2675, 12487.16375]\n",
      "\n",
      "Epoch: 110, NLL Loss: 557555.16471875, Val Loss: 17973.01953125, Time took: 1.2083139419555664\n",
      "Train loss Meta Info:  [113.566921875, 38.366203125]\n",
      "Val Loss Meta Info:  [5671.300625, 12301.7225]\n",
      "\n",
      "Epoch: 111, NLL Loss: 540480.416015625, Val Loss: 17796.203125, Time took: 1.2846779823303223\n",
      "Train loss Meta Info:  [113.5680625, 38.3074375]\n",
      "Val Loss Meta Info:  [5671.25875, 12124.94375]\n",
      "\n",
      "Epoch: 112, NLL Loss: 524245.062375, Val Loss: 17627.5390625, Time took: 1.2653179168701172\n",
      "Train loss Meta Info:  [113.566375, 38.2546796875]\n",
      "Val Loss Meta Info:  [5671.285, 11956.25375]\n",
      "\n",
      "Epoch: 113, NLL Loss: 508795.057625, Val Loss: 17466.427734375, Time took: 1.3319189548492432\n",
      "Train loss Meta Info:  [113.568296875, 38.20746484375]\n",
      "Val Loss Meta Info:  [5671.254375, 11795.1725]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, _ = generate_mpp(type='hawkes', num_sample=1000)\n",
    "val_data, _ = generate_mpp(type='hawkes', num_sample = 200)\n",
    "main(model=rmtpp, data=data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
