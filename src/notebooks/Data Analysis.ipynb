{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# raw_file = './../../data/dump/mimic.pickle'\n",
    "# with open(raw_file, 'rb') as handle:\n",
    "#     data = pickle.load(handle)\n",
    "# # filename = './../../data/dump/stack_processed.pickle'\n",
    "# # if os.path.isfile(filename):\n",
    "# #     with open(filename, 'rb') as handle:\n",
    "# #         print(\"Processed Dictionary Exists!\")\n",
    "# #         data = pickle.load(handle)\n",
    "# #         print(len(data))\n",
    "# # else:\n",
    "# #     data = process_elements(data)\n",
    "# #     with open(filename, 'wb') as handle:\n",
    "# #         pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(data))\n",
    "# print(len(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in data:\n",
    "#     for key, value in item.items():\n",
    "#         print(key, value)\n",
    "#         print()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for key, val in data.items():\n",
    "#     print(key,'\\n',val)\n",
    "#     print(len(val))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lengths of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(list(map(len, data.values())), bins=500)\n",
    "# plt.xlim(0,1000)\n",
    "# plt.xticks(np.linspace(0,1000,10, endpoint=True))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = './../../data/dump/data_meme/'\n",
    "# files = os.listdir(base_path)\n",
    "# file_list = [base_path+filename for filename in files if filename.split('.')[1]=='pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './../../data/'\n",
    "files = os.listdir(base_path)\n",
    "file_list = [base_path+filename for filename in files if filename.split('.')[0].startswith('hawkes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../../data/hawkesdatatest.txt',\n",
       " './../../data/hawkesdataval.txt',\n",
       " './../../data/hawkesdatatrain.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_stacked_time_array(x):\n",
    "    intervals = pandas.Series(x) - pandas.Series(x).shift(1)\n",
    "    intervals[0] = x[0]\n",
    "    return np.stack([intervals, pandas.Series(x)]).T\n",
    "\n",
    "def getdata_Hawkes(filepath):\n",
    "    t_data, x_data = [],[]\n",
    "    with open(filepath,'r') as f:\n",
    "        data=f.readlines()\n",
    "    # Split a line into list of floats\n",
    "    data = list(map(lambda x: list(map(float, str.split(x))), data))\n",
    "    t_data = list(map(list_to_stacked_time_array, data))\n",
    "    x_data = list(map(lambda x: np.ones(len(x)), data))\n",
    "    data_dict = {\n",
    "        't': t_data,\n",
    "        'x': x_data\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dict = getdata_Hawkes('./../../data/hawkesdataval.txt')\n",
    "train_dict = getdata_Hawkes('./../../data/hawkesdatatrain.txt')\n",
    "test_dict = getdata_Hawkes('./../../data/hawkesdatatest.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([26, 6, 43, 41, 25], [7, 30, 27, 17, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, valid_dict['t'][:5])), list(map(len, train_dict['t'][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_marker(seq):\n",
    "    \"\"\"\n",
    "        Input: dict with keys: 'time_since_start', 'time_since_last_event'\n",
    "    \"\"\"\n",
    "    seq = sorted(seq, key=lambda x: x['time_since_start'])\n",
    "    markers = np.array([datum['type_event'] for datum in seq])\n",
    "    return markers\n",
    "\n",
    "def seq_to_times(seq):\n",
    "    \"\"\"\n",
    "        Input: dict with keys: 'time_since_start', 'time_since_last_event'\n",
    "    \"\"\"\n",
    "    seq = sorted(seq, key=lambda x: x['time_since_start'])\n",
    "    times = np.array([(datum['time_since_last_event'], datum['time_since_start']) for datum in seq])\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2, 9, 4, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 2, 9, 4, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "idxs = random.sample(range(10), 5)\n",
    "print(idxs)\n",
    "np.arange(10)[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata_NHP(filepath, max_size=None):\n",
    "    t_data, x_data = [],[]\n",
    "    with open(filepath,'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "        for _data in data.values():\n",
    "            if (type(_data)==list) and len(_data)>0:\n",
    "                x_data.extend(list(map(seq_to_marker, _data)))\n",
    "                t_data.extend(list(map(seq_to_times, _data)))\n",
    "    if max_size is not None:\n",
    "        idxs = random.sample(range(len(x_data)), max_size)\n",
    "        t_data = [t_data[idx] for idx in idxs]\n",
    "        x_data = [x_data[idx] for idx in idxs]\n",
    "\n",
    "    data_dict = {\n",
    "        't': t_data,\n",
    "        'x': x_data\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getdata_NHP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a5d02a00c01c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdata_NHP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./../../data/hawkesdataval.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdata_NHP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./../../data/hawkesdatatrain.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdata_NHP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./../../data/hawkesdatatest.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getdata_NHP' is not defined"
     ]
    }
   ],
   "source": [
    "valid_dict = getdata_NHP('./../../data/dump/data_meme/dev.pkl')\n",
    "train_dict = getdata_NHP('./../../data/dump/data_meme/train.pkl')\n",
    "test_dict = getdata_NHP('./../../data/dump/data_meme/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125216.59"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(map(np.max, train_dict['t'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2000, 2000], [20000, 20000], [2000, 2000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, valid_dict.values())), list(map(len, train_dict.values())), list(map(len, test_dict.values())), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --job-name=hpp-lvm\n",
      "#SBATCH --partition m40-short\n",
      "#SBATCH --mem=4096\n",
      "#SBATCH --gres gpu:1\n",
      "#SBATCH --output=script-out-hpp.log\n",
      "\n",
      ". /home/abhishekshar/anaconda3/etc/profile.d/conda.sh\n",
      "conda activate hpp\n",
      "\n",
      "cd /home/abhishekshar/hierarchichal_point_process/src\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/abhishekshar/run_hpp.sh\", \"r\") as file:\n",
    "    boilerplate = file.read()\n",
    "\n",
    "print(boilerplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\t32\t64\n",
      "0.001\t64\t64\n",
      "0.001\t128\t64\n",
      "0.001\t32\t128\n",
      "0.001\t64\t128\n",
      "0.001\t128\t128\n",
      "0.001\t32\t256\n",
      "0.001\t64\t256\n",
      "0.001\t128\t256\n",
      "0.001\t32\t512\n",
      "0.001\t64\t512\n",
      "0.001\t128\t512\n",
      "0.001\t32\t1024\n",
      "0.001\t64\t1024\n",
      "0.001\t128\t1024\n"
     ]
    }
   ],
   "source": [
    "hiddenlayers = 2**np.arange(6,11)\n",
    "batch_sizes = [32,64,128]\n",
    "lr = 1e-3\n",
    "data_name = \"retweet\"\n",
    "filenames = []\n",
    "i = 0\n",
    "for hiddenlayer in hiddenlayers:\n",
    "    for batch_size in batch_sizes:\n",
    "        i += 1\n",
    "        print(\"{}\\t{}\\t{}\".format(lr, batch_size, hiddenlayer))\n",
    "        sbatch_command = \"python3 main.py --max_iter=100 --model=rmtpp --data_name={} --gamma=1. --l2=1e-3 --lr={} --batch_size={} --hidden_dim={}\".format(data_name, lr, batch_size, hiddenlayer)\n",
    "        filename = \"/home/abhishekshar/sbatch_hpp_{}_{}.sh\".format(data_name, i)\n",
    "        filenames.append(filename)\n",
    "        print(filename)\n",
    "        print(sbatch_command)\n",
    "        with open(filename,\"w+\") as file:\n",
    "            file.write(boilerplate)\n",
    "            file.write(sbatch_command)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/abhishekshar/{}_files\".format(data_name),\"w+\") as file:\n",
    "    for fname in filenames:\n",
    "        file.write(fname)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python3 main.py --max_iter=100 --model=rmtpp --data_name=meme --gamma=1. --l2=1e-3 --lr=0.001 --batch_size=128 --hidden_dim=1024'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbatch_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
